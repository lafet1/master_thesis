{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "import pickle\n",
    "import textblob\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "from textblob import TextBlob\n",
    "from processing_comment import ProcessingComment\n",
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, \\\n",
    "                                            CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train-balanced-sarcasm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1010768</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm sure that Iran and N. Korea have the techn...</td>\n",
       "      <td>TwarkMain</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-04</td>\n",
       "      <td>2009-04-25 00:47:52</td>\n",
       "      <td>No one is calling this an engineered pathogen,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010769</th>\n",
       "      <td>1</td>\n",
       "      <td>whatever you do, don't vote green!</td>\n",
       "      <td>BCHarvey</td>\n",
       "      <td>climate</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-05</td>\n",
       "      <td>2009-05-14 22:27:40</td>\n",
       "      <td>In a move typical of their recent do-nothing a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010770</th>\n",
       "      <td>1</td>\n",
       "      <td>Perhaps this is an atheist conspiracy to make ...</td>\n",
       "      <td>rebelcommander</td>\n",
       "      <td>atheism</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-01</td>\n",
       "      <td>2009-01-11 00:22:57</td>\n",
       "      <td>Screw the Disabled--I've got to get to Church ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010771</th>\n",
       "      <td>1</td>\n",
       "      <td>The Slavs got their own country - it is called...</td>\n",
       "      <td>catsi</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-01</td>\n",
       "      <td>2009-01-23 21:12:49</td>\n",
       "      <td>I've always been unsettled by that. I hear a l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010772</th>\n",
       "      <td>1</td>\n",
       "      <td>values, as in capitalism .. there is good mone...</td>\n",
       "      <td>frogking</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-01</td>\n",
       "      <td>2009-01-24 06:20:14</td>\n",
       "      <td>Why do the people who make our laws seem unabl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                            comment  \\\n",
       "1010768      1  I'm sure that Iran and N. Korea have the techn...   \n",
       "1010769      1                 whatever you do, don't vote green!   \n",
       "1010770      1  Perhaps this is an atheist conspiracy to make ...   \n",
       "1010771      1  The Slavs got their own country - it is called...   \n",
       "1010772      1  values, as in capitalism .. there is good mone...   \n",
       "\n",
       "                 author   subreddit  score  ups  downs     date  \\\n",
       "1010768       TwarkMain  reddit.com      2    2      0  2009-04   \n",
       "1010769        BCHarvey     climate      1    1      0  2009-05   \n",
       "1010770  rebelcommander     atheism      1    1      0  2009-01   \n",
       "1010771           catsi   worldnews      1    1      0  2009-01   \n",
       "1010772        frogking    politics      2    2      0  2009-01   \n",
       "\n",
       "                 created_utc  \\\n",
       "1010768  2009-04-25 00:47:52   \n",
       "1010769  2009-05-14 22:27:40   \n",
       "1010770  2009-01-11 00:22:57   \n",
       "1010771  2009-01-23 21:12:49   \n",
       "1010772  2009-01-24 06:20:14   \n",
       "\n",
       "                                            parent_comment  \n",
       "1010768  No one is calling this an engineered pathogen,...  \n",
       "1010769  In a move typical of their recent do-nothing a...  \n",
       "1010770  Screw the Disabled--I've got to get to Church ...  \n",
       "1010771  I've always been unsettled by that. I hear a l...  \n",
       "1010772  Why do the people who make our laws seem unabl...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256560\n",
      "(1010773, 11)\n"
     ]
    }
   ],
   "source": [
    "print(len(data.author.unique()))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "505368"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(data.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45338 256560\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(data.author.value_counts() > 4),\n",
    "      np.sum(data.author.value_counts() > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_try = TextBlob(data.iloc[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_try.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_spread(comment):\n",
    "    text = TextBlob(comment)\n",
    "    text = text.sentiment_assessments\n",
    "    list_ = []\n",
    "    if text[2]:\n",
    "        for i in range(len(text[2])):\n",
    "            list_.append(text[2][i][1])\n",
    "    else:\n",
    "        list_.append(0)\n",
    "    std = np.std(list_)\n",
    "    max_ = np.max(list_)\n",
    "    min_ = np.min(list_)\n",
    "    \n",
    "    return [text[0], text[1], max_, min_, std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_spread(data.iloc[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_try.sentiment_assessments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bag of words + lexical clues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments, parent_comments = data.iloc[:, 1], data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                 NC and NH.\n",
       "1          You do know west teams play against west teams...\n",
       "2          They were underdogs earlier today, but since G...\n",
       "3          This meme isn't funny none of the \"new york ni...\n",
       "4                            I could use one of those tools.\n",
       "5          I don't pay attention to her, but as long as s...\n",
       "6              Trick or treating in general is just weird...\n",
       "7                            Blade Mastery+Masamune or GTFO!\n",
       "8          You don't have to, you have a good build, buy ...\n",
       "9                          I would love to see him at lolla.\n",
       "10         I think a significant amount would be against ...\n",
       "11                            Damn I was hoping God was real\n",
       "12                                      They have an agenda.\n",
       "13                                               Great idea!\n",
       "14         Ayy bb wassup, it makes a bit more sense in co...\n",
       "15                                             what the fuck\n",
       "16                                                    noted.\n",
       "17         because it's what really bothers him... and it...\n",
       "18                             why you fail me, my precious?\n",
       "19                            Pre-Flashpoint Clark and Lois.\n",
       "20         She hugs him back tightly, burying her head in...\n",
       "21         At this point they're so stable I could build ...\n",
       "22         Conservatism as an ideology is for sure a reac...\n",
       "23         Maybe not control, but certainly that is evide...\n",
       "24         Mine auto renewed without asking me the other ...\n",
       "25                                                       466\n",
       "26                             Jesus is a FNAF fan confirmed\n",
       "27                                   This would make me cry.\n",
       "28         At first I thought it was instructions on fixi...\n",
       "29         This guy, there's no way he isn't trolling, ri...\n",
       "                                 ...                        \n",
       "1010743                                                ZOMG!\n",
       "1010744    Clearly the death sentence would have avoided ...\n",
       "1010745                                 Wow, that was quick.\n",
       "1010746                                            del *.xml\n",
       "1010747    I like the kid holding up the sign that says \"...\n",
       "1010748    Yes, and there's no such thing as mental illne...\n",
       "1010749    Thank you, Glen Beck, Rush Limbaugh, Sean Hann...\n",
       "1010750                                you and your facts...\n",
       "1010751                                             so cool.\n",
       "1010752               What fine, upstanding young gentlemen.\n",
       "1010753                                 Good luck with that.\n",
       "1010754    The real question is why God hasn't killed Bar...\n",
       "1010755    Women shouldn't lead men anyway... it's in the...\n",
       "1010756    Being in a region that is that hot and being f...\n",
       "1010757                              but he's totally racist\n",
       "1010758                                    Thank you unions.\n",
       "1010759    Foxnews is the most accurate reporting service...\n",
       "1010760                            OMG, WHAT'S NEXT, KISSES?\n",
       "1010761    nono, he'll go back to 1985 to stop the Syrian...\n",
       "1010762                   Who said I didn't have a big dick?\n",
       "1010763                                        forgot to add\n",
       "1010764    So *that's* why I can point my finger and have...\n",
       "1010765                   OH SWEET ANOTHER GUITAR HERO CLONE\n",
       "1010766               oh wow, I have never seen this before.\n",
       "1010767                                                   :O\n",
       "1010768    I'm sure that Iran and N. Korea have the techn...\n",
       "1010769                   whatever you do, don't vote green!\n",
       "1010770    Perhaps this is an atheist conspiracy to make ...\n",
       "1010771    The Slavs got their own country - it is called...\n",
       "1010772    values, as in capitalism .. there is good mone...\n",
       "Name: comment, Length: 1010773, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = TextBlob(comments[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Great idea!'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat1 = r'[.?!\",]+'\n",
    "pat2 = r'[!?]+'\n",
    "pat3 = r'\\.{3,10}'\n",
    "match1 = re.findall(pat1, ' aa bb cc ... dd . ee .. ff !?! r ?')\n",
    "match2 = re.findall(pat2, ' aa bb cc ... dd . ee .. ff !?! r ?')\n",
    "match3 = re.findall(pat3, ' aa bb cc ... dd . ee .. ff !?! r ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(match1, match2, match3)\n",
    "print(len(match1), len(match2), \n",
    "      len(match3)) # to get overall number of punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_string = ''.join(map(str, comments[:10000]))\n",
    "print(one_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = r'((?::|x|X|;|=)(?:-)?(?:\\)|D|P|p|X|/))'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_match = re.findall(emoji_pattern, one_string)\n",
    "all_emoji = [emoji for emoji in emoji_match \n",
    "             if emoji not in ['XP', 'xp', 'Xp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in testing.tags:\n",
    "    if len(word[0]) > 2:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testing.lower()) # decapitalize\n",
    "print(len(testing)) # get overall length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing.words[8].lemmatize('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in testing.tags:\n",
    "    if word[1].startswith('V'):\n",
    "        word = (word[0], 'v')\n",
    "        print(word)\n",
    "    elif word[1].startswith('N'):\n",
    "        word = (word[0], 'n')\n",
    "        print(word)\n",
    "    elif word[1].startswith('J'):\n",
    "        word = (word[0], 'a')\n",
    "        print(word)\n",
    "    elif word[1].startswith('R'):\n",
    "        word = (word[0], 'r')\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words = []\n",
    "for i in range(len(testing.words)):\n",
    "    word = testing.words[i]\n",
    "    if word.isalpha():\n",
    "        clean_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = list(get_stop_words('en'))         #About 900 stopwords\n",
    "nltk_words = list(stopwords.words('english')) #About 150 stopwords\n",
    "stops.extend(nltk_words)\n",
    "filtered_words = [word for word in testing.words \n",
    "                  if word not in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_pat = r'[A-Z]+'\n",
    "cap_pat2 = r'[A-Z]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps = re.findall(cap_pat, comments[11])\n",
    "caps2 = re.findall(cap_pat2, comments[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(caps2))\n",
    "max_len = 0\n",
    "for i in caps: # put somtething like length of comment/# of capitals\n",
    "    max_len = max(max_len, len(i))\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### actual functions after prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_bow_features(data):\n",
    "    ''' Creates a raw text for BoW and PoS tags.\n",
    "    \n",
    "    This function takes as input the vector of comments and extracts\n",
    "    the BoW and PoS tags. It creates two lists of lists which \n",
    "    are then returned.\n",
    "    Before the actual extraction starts stopwords are created. \n",
    "    Then a for loop goes through each comment and cleans the comment. \n",
    "    Afterwards the cleaned text and its tags are added to their\n",
    "    respective comments.\n",
    "    '''\n",
    "    \n",
    "    # final output\n",
    "    bow = []\n",
    "    pos = []\n",
    "    \n",
    "    # stopwords\n",
    "    stops = list(get_stop_words('en'))\n",
    "    nltk_words = list(stopwords.words('english'))\n",
    "    stops.extend(nltk_words)\n",
    "    \n",
    "    for j, comment in enumerate(data):\n",
    "        # counter\n",
    "        if j % 10000 == 0:\n",
    "            print(j)\n",
    "        \n",
    "        ##### comment cleaning\n",
    "        # textblob and lowercase\n",
    "        text = TextBlob(comment)\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # numbers + punctuation\n",
    "        clean_words = []\n",
    "        for word in text_lower.words:\n",
    "            word = word.stem()\n",
    "            if word.isalpha():\n",
    "                clean_words.append(word)\n",
    "        \n",
    "        # stopwords + short words\n",
    "        filtered_words = [word for word in clean_words\n",
    "                          if word not in stops]\n",
    "        final_list = []\n",
    "        for i in range(len(filtered_words)):\n",
    "            if len(filtered_words[i]) > 2:\n",
    "                final_list.append(filtered_words[i])\n",
    "        final_list = ' '.join(map(str, final_list))\n",
    "        \n",
    "        # pos tags\n",
    "        pos_tags = [text.pos_tags[k][1]\n",
    "                    for k in range(len(text.pos_tags))]\n",
    "        pos_list = ' '.join(map(str, pos_tags))\n",
    "        \n",
    "        bow.append(final_list)\n",
    "        pos.append(pos_list)\n",
    "    \n",
    "    return bow, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_and_lexical_features(data):\n",
    "    ''' Creates a raw text for BoW, PoS tags and a set of lexical features.\n",
    "    \n",
    "    This function takes as input the vector of comments and extracts\n",
    "    the BoW, PoS tags and all the lexical features. It creates three lists\n",
    "    of lists which are then returned.\n",
    "    Before the actual extraction starts stopwords and regex expressions\n",
    "    are created. Then a for loop goes through each comment and finds the\n",
    "    regex matches and cleans the comment. Afterwards the regex matches are\n",
    "    added to one of the list, the cleaned text and its tags are added to\n",
    "    respective comments as well.\n",
    "    '''\n",
    "    \n",
    "    # final output\n",
    "    lexical = []\n",
    "    bow = []\n",
    "    pos = []\n",
    "    \n",
    "    # stopwords\n",
    "    stops = list(get_stop_words('en'))\n",
    "    nltk_words = list(stopwords.words('english'))\n",
    "    stops.extend(nltk_words)\n",
    "    \n",
    "    # patterns for matching\n",
    "    cap_pat = r'[A-Z]+'\n",
    "    cap_pat2 = r'[A-Z]'\n",
    "    pat1 = r'[.?!\",]+'\n",
    "    pat2 = r'[!?]+'\n",
    "    pat3 = r'\\.{3,10}'\n",
    "    emoji_pattern = r'((?::|x|X|;|=)(?:-)?(?:\\)|D|P|p|X|/))'\n",
    "    \n",
    "    for j, comment in enumerate(data):\n",
    "        # counter\n",
    "        if j % 10000 == 0:\n",
    "            print(j)\n",
    "            \n",
    "        ###### lexical cues\n",
    "        # capitals\n",
    "        caps = re.findall(cap_pat, comment)\n",
    "        max_len = 0\n",
    "        for i in caps:\n",
    "            max_len = max(max_len, len(i))\n",
    "        caps2 = re.findall(cap_pat2, comment)\n",
    "        \n",
    "        # punctuation\n",
    "        match1 = re.findall(pat1, comment)\n",
    "        match2 = re.findall(pat2, comment)\n",
    "        match3 = re.findall(pat3, comment)\n",
    "        \n",
    "        # emojis\n",
    "        emoji_match = re.findall(emoji_pattern, comment)\n",
    "        all_emoji = [emoji for emoji in emoji_match \n",
    "                     if emoji not in ['XP', 'xp', 'Xp']]\n",
    "        \n",
    "        # length\n",
    "        length = len(comment)\n",
    "        len_vars = [len(caps) / length, len(caps2) / length, \n",
    "                    len(match1) / length, len(match2) / length, \n",
    "                    len(match3) / length, len(all_emoji) / length]\n",
    "        \n",
    "        ##### comment cleaning\n",
    "        # textblob and lowercase\n",
    "        text = TextBlob(comment)\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # numbers + punctuation\n",
    "        clean_words = []\n",
    "        for word in text_lower.words:\n",
    "            word = word.stem()\n",
    "            if word.isalpha():\n",
    "                clean_words.append(word)\n",
    "        \n",
    "        # stopwords + short words\n",
    "        filtered_words = [word for word in clean_words\n",
    "                          if word not in stops]\n",
    "        final_list = []\n",
    "        for i in range(len(filtered_words)):\n",
    "            if len(filtered_words[i]) > 2:\n",
    "                final_list.append(filtered_words[i])\n",
    "        final_list = ' '.join(map(str, final_list))\n",
    "        \n",
    "        # pos tags\n",
    "        pos_tags = [text.pos_tags[k][1]\n",
    "                    for k in range(len(text.pos_tags))]\n",
    "        pos_list = ' '.join(map(str, pos_tags))\n",
    "        \n",
    "        lexical.append(len_vars)\n",
    "        bow.append(final_list)\n",
    "        pos.append(pos_list)\n",
    "    \n",
    "    return lexical, bow, pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a bit of prototyping again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = TextBlob(comments[2247])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = [text.pos_tags[k][1]\n",
    "        for k in range(len(text.pos_tags))]\n",
    "pos_list = ' '.join(map(str, pos_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical, bow, pos = bow_and_lexical_features(comments[2249])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### actual functions again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bow_classic(array_bow, array_pos):\n",
    "    ''' Takes the BoW and PoS output and returns a BoW.'''\n",
    "    vec = CountVectorizer()\n",
    "    X = vec.fit_transform(array_bow)\n",
    "\n",
    "    vec_pos = CountVectorizer()\n",
    "    X_pos = vec_pos.fit_transform(array_pos)\n",
    "    \n",
    "    return X, X_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bow(array_bow, array_pos):\n",
    "    ''' Takes the BoW and PoS output and returns a normalized BoW.'''\n",
    "    vec = TfidfVectorizer()\n",
    "    X = vec.fit_transform(array_bow)\n",
    "\n",
    "    vec_pos = TfidfVectorizer()\n",
    "    X_pos = vec_pos.fit_transform(array_pos)\n",
    "    \n",
    "    return X, X_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bow2(array_bow):\n",
    "    ''' Takes the BoW and returns a normalized BoW.'''\n",
    "    vec = TfidfVectorizer()\n",
    "    X = vec.fit_transform(array_bow)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## similarity measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading spaCy model, the vanilla version which comes with spacy is enough\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_parent = nlp(parent_comments[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens_parent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp(comments[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.similarity(nlp(parent_comments[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment_similarity(comment, parent_comment):\n",
    "    simil = comment.similarity(parent_comment)\n",
    "    return simil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_similarity(tokens, tokens_parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, \n",
    "          token.vector_norm, token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[4].text == tokens[8].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### actual function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(tokens):\n",
    "    '''Computes maximum and minimum similarity within a comment.\n",
    "    \n",
    "    The input is a transformed comment by spaCy, output is a list of lists\n",
    "    of the similarities. The first for loop keeps only words which are\n",
    "    verbs or nouns, the second and third then calculate the similarity\n",
    "    between all verbs/nouns and the maximum and minimum of all these values\n",
    "    is returned.\n",
    "    '''\n",
    "    list_nouns = []\n",
    "    list_verbs = []\n",
    "    used_words = []\n",
    "    for token in tokens:\n",
    "        tag = token.tag_\n",
    "        if token.text in used_words:\n",
    "            None\n",
    "        else:\n",
    "            used_words.append(token.text)\n",
    "            if tag.startswith(\"V\"):\n",
    "                list_verbs.append(token)\n",
    "            elif tag.startswith('N'):\n",
    "                list_nouns.append(token)\n",
    "\n",
    "    simil_list = []\n",
    "    for k in range(len(list_nouns)):\n",
    "        new = [list_nouns[i] for i in range(len(list_nouns)) \n",
    "               if i != k]\n",
    "        for other_word in new:\n",
    "            if len(other_word) > 1:\n",
    "                simil_list.append(list_nouns[k] \\\n",
    "                                  .similarity(other_word))\n",
    "    similarity_nouns = list(set(simil_list))\n",
    "    \n",
    "    simil_list = []\n",
    "    for k in range(len(list_verbs)):\n",
    "        new = [list_verbs[i] for i in range(len(list_verbs)) \n",
    "               if i != k]\n",
    "        for other_word in new:\n",
    "            if len(other_word) > 1:\n",
    "                simil_list.append(list_verbs[k] \\\n",
    "                                  .similarity(other_word))\n",
    "    similarity_verbs = list(set(simil_list))\n",
    "    \n",
    "    # if no verb/noun is present\n",
    "    if not similarity_nouns:\n",
    "        similarity_nouns.append(0)\n",
    "    if not similarity_verbs:\n",
    "        similarity_verbs.append(0)\n",
    "    \n",
    "    return [np.max(similarity_nouns), np.min(similarity_nouns), \n",
    "            np.max(similarity_verbs), np.min(similarity_verbs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similarity(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similarity(tokens_parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wrapper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sentiment_spread - one comment\n",
    "bow_and_lexical_features - vector of comments\n",
    "create_bow - output from bow_and_lexical_features\n",
    "get_similarity - tokens, comment transformed by nlp() command\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_all_data(data):\n",
    "    '''Wrapper for sentiment_spread(), input: whole data set'''\n",
    "    comment_sent = []\n",
    "    parent_sent = []\n",
    "    for index, row in data.iterrows():\n",
    "        if index % 1000 == 0:\n",
    "            print(index)\n",
    "        comment_sent.append(sentiment_spread(row['comment']))\n",
    "        parent_sent.append(sentiment_spread(row['parent_comment']))\n",
    "    \n",
    "    return comment_sent, parent_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the sentiment_all_data() function applied to both parent and original\n",
    "# comments and the result saved for later use\n",
    "orig_sentiment, par_sentiment = sentiment_all_data(data)\n",
    "pickle.dump(orig_sentiment, open(\"orig_sentiment\", \"wb\"),\n",
    "           protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(par_sentiment, open(\"par_sentiment\", \"wb\"),\n",
    "           protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lex_bow_pos_all(data):\n",
    "    ''' Gets cleaned text, PoS and lexical clues from data set'''\n",
    "    lexical, bow, pos = bow_and_lexical_features(data['comment'])\n",
    "    lexical_par, bow_par, pos_par = bow_and_lexical_features(\n",
    "        data['parent_comment'])\n",
    "    \n",
    "    return lexical, lexical_par, bow, pos, bow_par, pos_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lex_bow_pos_all_2(bow, pos, bow_par, pos_par):\n",
    "    ''' Creates BoW and PoS (tfidf)matrix from cleaned text and PoS tags'''\n",
    "    bow_matrix, pos_matrix = create_bow(bow, pos)\n",
    "    bow_matrix_par, pos_matrix_par = create_bow(bow_par, pos_par)\n",
    "    \n",
    "    return bow_matrix, pos_matrix, bow_matrix_par, pos_matrix_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_pos_all(data):\n",
    "    ''' Gets cleaned text and PoS from data set'''\n",
    "    bow_matrix, pos_matrix = create_bow(bow, pos)\n",
    "    bow_matrix_par, pos_matrix_par = create_bow(bow_par, pos_par)\n",
    "    \n",
    "    return bow_matrix, pos_matrix, bow_matrix_par, pos_matrix_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_pos_all_2(bow, pos, bow_par, pos_par):\n",
    "    ''' Creates BoW and PoS matrix from cleaned text and PoS tags'''\n",
    "    bow_matrix, pos_matrix = create_bow_classic(bow, pos)\n",
    "    bow_matrix_par, pos_matrix_par = create_bow_classic(bow_par, \n",
    "                                                        pos_par)\n",
    "    \n",
    "    return bow_matrix, pos_matrix, bow_matrix_par, pos_matrix_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the lex_bow_pos_all() function applied to both parent and original\n",
    "# comments and the result saved for later use\n",
    "lexical, lexical_parent, bow, pos, bow_parent, \\\n",
    "        pos_parent = lex_bow_pos_all(data)\n",
    "\n",
    "pickle.dump(lexical, open(\"lexical\", \"wb\"),\n",
    "           protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(lexical_parent, open(\"lexical_parent\", \"wb\"),\n",
    "           protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(bow, open(\"bow\", \"wb\"),\n",
    "           protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(pos, open(\"pos\", \"wb\"),\n",
    "           protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(bow_parent, open(\"bow_parent\", \"wb\"),\n",
    "           protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(pos_parent, open(\"pos_parent\", \"wb\"),\n",
    "           protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the lex_bow_pos_all_2() function applied to both parent and original\n",
    "# comments and the result saved for later use\n",
    "bow_matrix, pos_matrix, bow_matrix_par, pos_matrix_par = \\\n",
    "    lex_bow_pos_all_2(bow, pos, bow_parent, pos_parent)\n",
    "\n",
    "pickle.dump(bow_matrix, open(\"bow_matrix\", \"wb\"),\n",
    "           protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(pos_matrix, open(\"pos_matrix\", \"wb\"),\n",
    "           protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(bow_matrix_par, open(\"bow_matrix_par\", \"wb\"),\n",
    "           protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(pos_matrix_par, open(\"pos_matrix_par\", \"wb\"),\n",
    "           protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the bow_pos_all() function applied to cleaned data from par and orig\n",
    "# comments and the result saved for later use\n",
    "bow_cl, pos_cl, bow_parent_cl, pos_parent_cl = bow_pos_all(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the bow_pos_all_2() function applied to cleaned data from par and orig\n",
    "# comments and the result saved for later use\n",
    "bow_matrix_cl, pos_matrix_cl, bow_matrix_par_cl, pos_matrix_par_cl = \\\n",
    "    bow_pos_all_2(bow_cl, pos_cl, bow_parent_cl, pos_parent_cl)\n",
    "\n",
    "pickle.dump(bow_matrix_cl, open(\"bow_matrix_cl\", \"wb\"),\n",
    "           protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(pos_matrix_cl, open(\"pos_matrix_cl\", \"wb\"),\n",
    "           protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(bow_matrix_par_cl, open(\"bow_matrix_par_cl\", \"wb\"),\n",
    "           protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(pos_matrix_par_cl, open(\"pos_matrix_par_cl\", \"wb\"),\n",
    "           protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_all(data):\n",
    "    '''Wrapper for get_similarity() function, input entire data set'''\n",
    "    comment_sim = []\n",
    "    parent_sim = []\n",
    "    to_par_sim = []\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        if index % 1000 == 0:\n",
    "            print(index)\n",
    "        comment = nlp(row['comment'])\n",
    "        par_comment = nlp(row['parent_comment'])\n",
    "        comment_sim.append(get_similarity(comment))\n",
    "        parent_sim.append(get_similarity(par_comment))\n",
    "        to_par_sim.append(comment.similarity(par_comment))\n",
    "    \n",
    "    return comment_sim, parent_sim, to_par_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the similarity_all() function applied to cleaned data from par and orig\n",
    "# comments and the result saved for later use\n",
    "orig_simil, par_simil, com_to_par_sim = similarity_all(data)\n",
    "\n",
    "pickle.dump(orig_simil, open(\"orig_simil\", \"wb\"),\n",
    "           protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(par_simil, open(\"par_simil\", \"wb\"),\n",
    "           protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(com_to_par_sim, open(\"com_to_par_sim\", \"wb\"),\n",
    "           protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## user embeddings based measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_[0]['Always_the_NewGuy'][0].reshape(-1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(list_[0]['Always_the_NewGuy'],\n",
    "       list_[0]['Always_smooth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/user_embeddings/AmazingAlo\",'rb') as df:\n",
    "    list_ = pickle.load(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_[0]['Always_smooth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smth = nlp(data['comment'][5:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(smth.tensor, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[555, 'author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_index = list(data[data['author'] == 'Trumpbart'].index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data.iloc[:10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.loc[sub_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_np = np.zeros((data.shape[0], 1))\n",
    "test_np[sub_index] = np.array((1, 3)).reshape((-1, 1))\n",
    "sum(test_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### actual function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_deviation(d):\n",
    "    '''Takes user embedding and calculates the comment's similarity\n",
    "    \n",
    "    Input is the data set with the comments, then every user's\n",
    "    embedding is retrieved, data set subsetted, the similarity\n",
    "    calculated and a numpy array changed. The output is the numpy\n",
    "    array with the similarities.\n",
    "    '''\n",
    "    \n",
    "    user_simil = np.zeros((d.shape[0], 1))\n",
    "    dicts = os.listdir('data/user_embeddings')\n",
    "    for dict_ in dicts:\n",
    "        with open('data/user_embeddings/' + dict_, 'rb') as df:\n",
    "            list_ = pickle.load(df)\n",
    "        n_lists = len(list_)\n",
    "        print(dict_)\n",
    "        \n",
    "        for j in range(n_lists):\n",
    "            users = list_[j].keys()\n",
    "            for user in users:\n",
    "                subset = d.query('author == @user')['comment'] \\\n",
    "                            .apply(nlp)\n",
    "                index_ = list(subset.index.values)\n",
    "                com_tensors = [np.sum(comment.tensor, axis=0)\n",
    "                               for comment in subset]             \n",
    "                simil = [cosine(tensor, list_[j][user])\n",
    "                         for tensor in com_tensors]\n",
    "                user_simil[index_] = np.array(simil) \\\n",
    "                                        .reshape((-1, 1))\n",
    "    \n",
    "    return user_simil   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_similarity = user_deviation(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(user_similarity, open(\"user_similarity\", \"wb\"),\n",
    "           protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevalence = data.groupby('subreddit')['label'].sum() \\\n",
    "    / data.groupby('subreddit')['label'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['07Scape', '0x10c', '0x3642', '100DaysofKeto', '100pushups',\n",
       "       '100thieves', '1022', '10cloverfieldlane', '10pm', '112263Hulu',\n",
       "       ...\n",
       "       'zombiemanic', 'zombies', 'zooeydeschanel', 'zookeeperbattle',\n",
       "       'zoology', 'zoophilia', 'zootopia', 'zweiteliga', 'zyramains', 'zyzz'],\n",
       "      dtype='object', name='subreddit', length=14876)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prevalence.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### actual function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prevalence_vector_ordered(d):\n",
    "    '''Calculates the prevalence of sarcasm and returns ordered vector\n",
    "    \n",
    "    Input is the whole data set and output one ordered vector.\n",
    "    The prevalence per subreddit is the first command, then an empty\n",
    "    vector is created and then filled with a for loop.\n",
    "    '''\n",
    "    prevalence = d.groupby('subreddit')['label'].sum() \\\n",
    "        / d.groupby('subreddit')['label'].count()\n",
    "    \n",
    "    prevalence_vector = np.zeros((d.shape[0], 1))\n",
    "    subreddits = d['subreddit'].unique()\n",
    "    for subreddit in subreddits:\n",
    "        # subset for the specific subreddit is created\n",
    "        subset = d.query('subreddit == @subreddit')\n",
    "        # prevalence vector is subsetted and a new auxilliary one created\n",
    "        prevalence_subset = prevalence[prevalence.index == subreddit]\n",
    "        index2 = list(prevalence_subset.index.values)\n",
    "        subreddit_prevalence = prevalence[index2]\n",
    "        # original indexes of this subset are saved and used to subset\n",
    "        # the fill the final vector\n",
    "        index1 = list(subset.index.values)\n",
    "        prevalence_vector[index1] = np.array(subreddit_prevalence) \\\n",
    "                                        .reshape((-1, 1))\n",
    "    \n",
    "    return prevalence_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "smth = prevalence_vector_ordered(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1010773, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(smth, open(\"subreddit_prevalence\", \"wb\"),\n",
    "           protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
